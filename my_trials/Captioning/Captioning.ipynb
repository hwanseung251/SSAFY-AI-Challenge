{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702501c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# 모델 로드 \n",
    "CAPTION_MODEL_ID = \"Salesforce/blip-image-captioning-large\"\n",
    "caption_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "caption_processor = BlipProcessor.from_pretrained(CAPTION_MODEL_ID)\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\n",
    "    CAPTION_MODEL_ID,\n",
    "    torch_dtype=torch.float16 if caption_device == \"cuda\" else torch.float32,\n",
    ").to(caption_device).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def caption_image(pil_img, max_new_tokens: int = 40, resize_max: int = 1024):\n",
    "    # 너무 큰 이미지면 살짝 줄여 캡션 속도/메모리 절약\n",
    "    if resize_max is not None:\n",
    "        pil_img = pil_img.copy()\n",
    "        pil_img.thumbnail((resize_max, resize_max))\n",
    "\n",
    "    inputs = caption_processor(images=pil_img, return_tensors=\"pt\").to(caption_device)\n",
    "    out_ids = caption_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    text = caption_processor.tokenizer.decode(out_ids[0], skip_special_tokens=True).strip()\n",
    "    if text and text[-1] not in \".!?\":\n",
    "        text += \".\"\n",
    "    return text\n",
    "\n",
    "def one_sentence(text: str):\n",
    "    # 필요 시 한 문장만 쓰고 싶을 때 사용(선택)\n",
    "    idx = text.find(\".\")\n",
    "    return text[:idx+1] if idx != -1 else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd988af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# BASE_DIR\n",
    "BASE_DIR = \"/content/drive/MyDrive/AI챌린지/2025-ssafy-14\"\n",
    "\n",
    "def augment_questions_inplace(df, base_dir=BASE_DIR, store_depiction_col: bool = True, one_sentence_only: bool = False):\n",
    "    depictions = []\n",
    "    orig_questions = df[\"question\"].astype(str).tolist()\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Captioning\"):\n",
    "        rel_path = str(row[\"path\"]).strip()\n",
    "        img_path = os.path.join(base_dir, rel_path)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        dep = caption_image(img)  # 영어 캡션\n",
    "        if one_sentence_only:\n",
    "            dep = one_sentence(dep)\n",
    "\n",
    "        depictions.append(dep)\n",
    "\n",
    "    if store_depiction_col:\n",
    "        df[\"depiction\"] = depictions  # 참고용(원치 않으면 False)\n",
    "\n",
    "    # 기존 question 덮어쓰기\n",
    "    df[\"question\"] = [f\"This picture seems like {d} {q}\" for d, q in zip(depictions, orig_questions)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42598cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test 모두 동일 적용\n",
    "train_df = augment_questions_inplace(train_df, BASE_DIR, store_depiction_col=False, one_sentence_only=False)\n",
    "test_df  = augment_questions_inplace(test_df,  BASE_DIR, store_depiction_col=False, one_sentence_only=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
